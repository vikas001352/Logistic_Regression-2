{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfffdde-6756-4367-918e-4e86b718cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "\n",
    "ANS-1\n",
    "\n",
    "\n",
    "Grid Search CV (Cross-Validation) is a technique used in machine learning to find the best combination of hyperparameters for a given model. Hyperparameters are parameters that are set before the model is trained and cannot be learned from the data, unlike model parameters (coefficients or weights), which are learned during training.\n",
    "\n",
    "The purpose of Grid Search CV is to systematically search through a predefined set of hyperparameter values and evaluate the model's performance using cross-validation to identify the hyperparameter combination that results in the best performance.\n",
    "\n",
    "Here's how Grid Search CV works:\n",
    "\n",
    "1. Define the Hyperparameter Grid:\n",
    "First, you need to define a grid of hyperparameter values for each hyperparameter you want to tune. For example, if you are using a Support Vector Machine (SVM) model, you might want to tune the regularization parameter C and the kernel type. You could define a range of C values (e.g., [0.1, 1, 10]) and a set of kernel types (e.g., ['linear', 'rbf']) to create a grid of hyperparameter combinations.\n",
    "\n",
    "2. Create Model Instances:\n",
    "For each hyperparameter combination in the grid, create an instance of the model with those specific hyperparameter values.\n",
    "\n",
    "3. Cross-Validation:\n",
    "Perform k-fold cross-validation on each model instance using the training data. In k-fold cross-validation, the training data is divided into k subsets (folds), and the model is trained on k-1 folds and evaluated on the remaining fold. This process is repeated k times, with each fold serving as the validation set exactly once. The evaluation metric (e.g., accuracy, F1-score, AUC) is recorded for each fold.\n",
    "\n",
    "4. Average Performance:\n",
    "Calculate the average performance metric across all k folds for each model instance. This gives a more robust estimate of the model's performance than evaluating on a single validation set.\n",
    "\n",
    "5. Select the Best Model:\n",
    "Compare the average performance of each model instance (corresponding to a specific hyperparameter combination). The hyperparameter combination that yields the highest performance score is selected as the best model.\n",
    "\n",
    "6. Retrain on Full Training Set:\n",
    "Once the best hyperparameter combination is determined, the model is retrained on the entire training dataset using those optimal hyperparameter values.\n",
    "\n",
    "7. Evaluate on Test Set:\n",
    "Finally, the model's performance is evaluated on an independent test set that was not used during the hyperparameter tuning process. This provides an unbiased estimate of the model's performance on unseen data.\n",
    "\n",
    "Grid Search CV helps automate the process of finding the best hyperparameters for a model, saving time and effort compared to manually tuning hyperparameters. It systematically explores different combinations of hyperparameter values and selects the one that maximizes the model's performance. However, Grid Search CV can be computationally expensive, especially when the hyperparameter grid is large or the dataset is large. In such cases, other techniques like Randomized Search or Bayesian Optimization may be used to efficiently search the hyperparameter space.\n",
    "\n",
    "\n",
    "\n",
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?\n",
    "\n",
    "\n",
    "ANS-2\n",
    "\n",
    "\n",
    "Grid Search CV and Randomized Search CV are both techniques used for hyperparameter tuning in machine learning, but they differ in how they search through the hyperparameter space. Let's discuss the differences between the two and when to choose one over the other:\n",
    "\n",
    "1. Grid Search CV:\n",
    "   - Grid Search CV exhaustively searches through all the combinations of hyperparameter values defined in a predefined grid.\n",
    "   - It evaluates the model's performance for each combination using cross-validation and selects the hyperparameter combination with the best performance.\n",
    "   - Grid Search CV covers the entire search space, testing every possible combination, which makes it more thorough but potentially computationally expensive, especially with a large number of hyperparameters and their possible values.\n",
    "   - It is suitable when the hyperparameter search space is relatively small, and computational resources are not a constraint. It is ideal when you have a good understanding of the hyperparameters that need to be tuned and their possible values.\n",
    "\n",
    "2. Randomized Search CV:\n",
    "   - Randomized Search CV randomly samples a fixed number of hyperparameter combinations from the defined search space.\n",
    "   - It allows you to specify the number of iterations, and in each iteration, it randomly selects a set of hyperparameters from the predefined distributions or ranges.\n",
    "   - Randomized Search CV does not cover the entire search space like Grid Search, but it can efficiently explore a larger portion of the space in fewer iterations.\n",
    "   - It is particularly useful when the hyperparameter search space is extensive, and conducting an exhaustive search through Grid Search CV would be computationally infeasible.\n",
    "   - Randomized Search CV provides a good balance between exploration and exploitation, as it explores various hyperparameter combinations while avoiding unnecessary evaluations of unpromising combinations.\n",
    "\n",
    "When to Choose Grid Search CV or Randomized Search CV:\n",
    "- Use Grid Search CV when you have a relatively small hyperparameter search space and computational resources are not a major concern. Grid Search CV ensures a thorough exploration of all possible hyperparameter combinations.\n",
    "- Choose Randomized Search CV when the hyperparameter search space is extensive, and conducting an exhaustive search through Grid Search CV would be computationally impractical. Randomized Search CV allows you to explore a broader range of hyperparameter combinations efficiently, even with a limited number of iterations.\n",
    "- If you have limited computational resources but still want to perform a hyperparameter search, Randomized Search CV is a more practical choice as it provides a good balance between thoroughness and efficiency.\n",
    "\n",
    "In summary, Grid Search CV is comprehensive but can be computationally expensive, while Randomized Search CV is more efficient for exploring extensive hyperparameter search spaces. The choice between the two depends on the available computational resources, the complexity of the hyperparameter search space, and the need for an exhaustive exploration of hyperparameter combinations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "\n",
    "ANS-3\n",
    "\n",
    "Data leakage, also known as information leakage, occurs when information from the test set or unseen data accidentally leaks into the training process, leading to overly optimistic model performance. It is a critical problem in machine learning because it can result in models that appear to perform very well during training and validation but fail to generalize to new, unseen data.\n",
    "\n",
    "Data leakage can arise from various sources, and it can severely impact the integrity and reliability of the model's performance evaluation and subsequent deployment. The main reasons data leakage occurs are:\n",
    "\n",
    "1. Train-Test Contamination: Information from the test set unintentionally finds its way into the training process. For example, using information from the test set to preprocess the training data, or training the model on data that contains timestamps beyond the test set's time frame.\n",
    "\n",
    "2. Target Leakage: When the features used for training the model contain information that directly or indirectly reveals the target variable. In this case, the model may implicitly learn patterns that are not generalizable to new data, leading to overfitting.\n",
    "\n",
    "3. Data Preprocessing Mistakes: Preprocessing steps, such as scaling or normalization, should be performed on the training data only and then applied to the test data separately. If these steps involve information from the test set, it can cause data leakage.\n",
    "\n",
    "4. Overfitting to Validation Data: When tuning hyperparameters or selecting the best model, using the validation set multiple times without proper cross-validation can lead to overfitting the hyperparameters to the validation data.\n",
    "\n",
    "Example of Data Leakage:\n",
    "\n",
    "Let's consider a credit card fraud detection problem. The dataset contains transaction details, including the transaction amount and whether the transaction is fraudulent or not. The goal is to build a model to predict fraudulent transactions accurately.\n",
    "\n",
    "Suppose the dataset contains a column indicating whether a transaction is labeled as \"reported fraud\" or not. Now, imagine that the model is used to predict fraud based on this \"reported fraud\" column. The model will likely have excellent performance because the \"reported fraud\" column is a direct indicator of fraud.\n",
    "\n",
    "However, using the \"reported fraud\" column as a feature in the model introduces target leakage since it reveals information about the target variable (fraud) that is only available after the transaction occurs. In a real-world scenario, we wouldn't have access to the \"reported fraud\" information at the time of making predictions. Consequently, when deploying the model, it will likely perform poorly because it was trained on data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "\n",
    "ANS-4\n",
    "\n",
    "\n",
    "Preventing data leakage is crucial when building a machine learning model to ensure the model's performance reflects its true generalization capabilities. Data leakage can occur when information from the test set or future data is inadvertently included in the training process, leading to overly optimistic results. To prevent data leakage, follow these best practices:\n",
    "\n",
    "1. **Data Splitting**: Divide your dataset into three distinct sets: training, validation, and test sets. The training set is used to train the model, the validation set is used for hyperparameter tuning and model selection, and the test set is reserved for final evaluation.\n",
    "\n",
    "2. **Time-based Splitting (for time-series data)**: If your data is time-dependent (e.g., financial data, stock prices), split the data chronologically. Train the model on past data, validate it on more recent data, and test it on the most recent data. This simulates real-world scenarios where the model is trained on historical data and tested on future data.\n",
    "\n",
    "3. **Feature Engineering**: Avoid using future information or information that would not be available in real-world predictions. Ensure that all the features used for training are obtainable before the prediction time.\n",
    "\n",
    "4. **Cross-Validation**: When using cross-validation for model evaluation, ensure that no data from the validation folds (or test fold in k-fold cross-validation) is used in any part of the training process. This helps in getting a more robust estimate of the model's performance.\n",
    "\n",
    "5. **Preprocessing and Scaling**: Apply feature scaling and other preprocessing steps separately on the training and validation/test sets. Calculating scaling factors based on the entire dataset can introduce data leakage.\n",
    "\n",
    "6. **Mean Imputation**: Avoid mean imputation or any other imputation methods that use global statistics (e.g., mean, median) calculated from the entire dataset. Instead, perform imputation separately on the training and validation/test sets.\n",
    "\n",
    "7. **Target Leakage**: Be cautious of features that directly or indirectly leak information about the target variable. For example, including features derived from future data can lead to target leakage.\n",
    "\n",
    "8. **Regularization**: Use regularization techniques like L1 (Lasso) or L2 (Ridge) regularization to prevent overfitting and make the model less sensitive to individual data points.\n",
    "\n",
    "9. **Careful Validation**: During the validation process, avoid any manual adjustments or feature selections based on validation performance. This can lead to unintentional data leakage.\n",
    "\n",
    "10. **Pipeline Design**: Use scikit-learn Pipelines or similar frameworks to ensure that preprocessing steps, feature engineering, and model training are done in a way that prevents data leakage.\n",
    "\n",
    "11. **Outliers**: Handle outliers carefully. Removing outliers based on global statistics calculated from the entire dataset can introduce data leakage. It's better to identify and remove outliers only from the training set.\n",
    "\n",
    "By following these best practices, you can minimize the risk of data leakage and build a more robust and reliable machine learning model.\n",
    "\n",
    "\n",
    "\n",
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "\n",
    "ANS-5\n",
    "\n",
    "\n",
    "A confusion matrix is a performance evaluation tool used to assess the performance of a classification model. It provides a summary of the predictions made by the model on a test dataset and compares them to the actual ground truth labels. The confusion matrix is particularly useful in problems where the data is imbalanced, meaning one class may dominate the dataset, as it provides more insights than simple accuracy.\n",
    "\n",
    "A typical confusion matrix for a binary classification problem looks like this:\n",
    "\n",
    "```\n",
    "                 Predicted Negative    Predicted Positive\n",
    "Actual Negative        TN                   FP\n",
    "Actual Positive        FN                   TP\n",
    "```\n",
    "\n",
    "Here's what each term in the confusion matrix means:\n",
    "\n",
    "- **True Positives (TP)**: The number of instances of the positive class correctly predicted by the model. These are the cases where the model correctly identified the positive samples.\n",
    "\n",
    "- **True Negatives (TN)**: The number of instances of the negative class correctly predicted by the model. These are the cases where the model correctly identified the negative samples.\n",
    "\n",
    "- **False Positives (FP)**: The number of instances of the negative class that were incorrectly predicted as positive by the model. Also known as Type I errors, these are cases where the model falsely identified a negative sample as positive.\n",
    "\n",
    "- **False Negatives (FN)**: The number of instances of the positive class that were incorrectly predicted as negative by the model. Also known as Type II errors, these are cases where the model falsely identified a positive sample as negative.\n",
    "\n",
    "Now, based on these values, several performance metrics can be calculated:\n",
    "\n",
    "1. **Accuracy**: It is the overall accuracy of the model and is calculated as `(TP + TN) / (TP + TN + FP + FN)`. It gives the proportion of correct predictions out of the total number of predictions.\n",
    "\n",
    "2. **Precision**: Precision is the ability of the model to correctly identify positive predictions among all predicted positive instances. It is calculated as `TP / (TP + FP)`.\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate)**: Recall measures the ability of the model to correctly identify positive instances among all actual positive instances. It is calculated as `TP / (TP + FN)`.\n",
    "\n",
    "4. **Specificity (True Negative Rate)**: Specificity measures the ability of the model to correctly identify negative instances among all actual negative instances. It is calculated as `TN / (TN + FP)`.\n",
    "\n",
    "5. **F1 Score**: The F1 score is the harmonic mean of precision and recall and provides a balanced measure between the two. It is calculated as `2 * (Precision * Recall) / (Precision + Recall)`.\n",
    "\n",
    "The confusion matrix allows you to gain deeper insights into the performance of your classification model by understanding where it is making correct or incorrect predictions and which classes are being affected. It helps in identifying if the model is biased towards one class, whether it's overfitting or underfitting, and which performance metric is most appropriate for your problem depending on its requirements.\n",
    "\n",
    "\n",
    "\n",
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "\n",
    "\n",
    "ANS-6\n",
    "\n",
    "\n",
    "In the context of a confusion matrix, precision and recall are two important performance metrics used to evaluate the performance of a classification model, particularly in binary classification problems. They are derived from the values in the confusion matrix and provide different insights into the model's performance with respect to positive class predictions.\n",
    "\n",
    "Let's revisit the confusion matrix for a binary classification problem:\n",
    "\n",
    "```\n",
    "                 Predicted Negative    Predicted Positive\n",
    "Actual Negative        TN                   FP\n",
    "Actual Positive        FN                   TP\n",
    "```\n",
    "\n",
    "**Precision**:\n",
    "Precision is the ability of the model to correctly identify positive predictions among all predicted positive instances. In other words, it tells us how many of the instances predicted as positive are actually positive.\n",
    "\n",
    "Precision is calculated as:\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "\n",
    "- True Positives (TP): The number of instances of the positive class correctly predicted by the model.\n",
    "- False Positives (FP): The number of instances of the negative class that were incorrectly predicted as positive by the model.\n",
    "\n",
    "Precision is important when the cost of false positives (Type I errors) is relatively high. For example, in a medical diagnosis scenario, false positives may lead to unnecessary treatments or interventions.\n",
    "\n",
    "**Recall (Sensitivity or True Positive Rate)**:\n",
    "Recall measures the ability of the model to correctly identify positive instances among all actual positive instances. It tells us what proportion of the actual positive instances the model has correctly classified as positive.\n",
    "\n",
    "Recall is calculated as:\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "- True Positives (TP): The number of instances of the positive class correctly predicted by the model.\n",
    "- False Negatives (FN): The number of instances of the positive class that were incorrectly predicted as negative by the model.\n",
    "\n",
    "Recall is important when the cost of false negatives (Type II errors) is relatively high. For example, in a disease detection scenario, false negatives may lead to missing critical cases that require immediate attention.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- Precision focuses on the correctness of positive predictions among all predicted positive instances. It is concerned with the question, \"Of all the instances predicted as positive, how many are actually positive?\"\n",
    "\n",
    "- Recall focuses on the completeness of positive predictions among all actual positive instances. It is concerned with the question, \"Of all the actual positive instances, how many did the model correctly predict as positive?\"\n",
    "\n",
    "The choice between precision and recall as the more important metric depends on the specific context and requirements of the problem. In some cases, you may prioritize precision, while in others, recall may be more critical. It's common to consider both precision and recall together, often by using the F1 score, which is the harmonic mean of precision and recall, to get a balanced view of the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "\n",
    "\n",
    "ANS-7\n",
    "\n",
    "\n",
    "\n",
    "Interpreting a confusion matrix allows you to gain valuable insights into the types of errors your classification model is making. By analyzing the values in the confusion matrix, you can understand where the model is performing well and where it is struggling. Let's use the standard binary classification confusion matrix as a reference:\n",
    "\n",
    "```\n",
    "                 Predicted Negative    Predicted Positive\n",
    "Actual Negative        TN                   FP\n",
    "Actual Positive        FN                   TP\n",
    "```\n",
    "\n",
    "Here's how you can interpret the confusion matrix to determine the types of errors your model is making:\n",
    "\n",
    "1. **True Positives (TP)**: These are the instances of the positive class that the model correctly predicted as positive. These are the cases where the model made correct positive predictions.\n",
    "\n",
    "2. **True Negatives (TN)**: These are the instances of the negative class that the model correctly predicted as negative. These are the cases where the model made correct negative predictions.\n",
    "\n",
    "3. **False Positives (FP)**: These are the instances of the negative class that the model incorrectly predicted as positive. These are also known as Type I errors. False positives occur when the model wrongly classifies a negative sample as positive.\n",
    "\n",
    "4. **False Negatives (FN)**: These are the instances of the positive class that the model incorrectly predicted as negative. These are also known as Type II errors. False negatives occur when the model wrongly classifies a positive sample as negative.\n",
    "\n",
    "Now, let's interpret the confusion matrix to identify the types of errors:\n",
    "\n",
    "- High **False Positive (FP) Rate**: If you see a relatively high number of false positives compared to true negatives (FP >> TN), it indicates that the model is incorrectly classifying many negative instances as positive. This means the model may be over-predicting the positive class.\n",
    "\n",
    "- High **False Negative (FN) Rate**: If you observe a relatively high number of false negatives compared to true positives (FN >> TP), it indicates that the model is incorrectly classifying many positive instances as negative. This means the model may be under-predicting the positive class.\n",
    "\n",
    "- Balanced Accuracy: If both the true positive rate (Recall) and true negative rate (Specificity) are relatively high, and false positive and false negative rates are low, it suggests that the model is performing well and is not biased towards one class.\n",
    "\n",
    "- **Imbalanced Dataset**: If you have an imbalanced dataset (one class has significantly more samples than the other), the model may perform well on the majority class (high TN) but poorly on the minority class (high FN). In such cases, it's important to look at precision, recall, and F1 score to get a comprehensive evaluation.\n",
    "\n",
    "By interpreting the confusion matrix, you can identify specific areas of improvement for your model. For example, if false positives are a concern, you might want to adjust the decision threshold or implement different class weights to reduce false positives. If false negatives are an issue, you might consider collecting more data for the minority class or using techniques like data augmentation or resampling. The interpretation of the confusion matrix helps guide you in fine-tuning your model to achieve better performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n",
    "\n",
    "\n",
    "\n",
    "ANS-8\n",
    "\n",
    "\n",
    "Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. These metrics provide valuable insights into how well the model is making predictions and can help you understand its strengths and weaknesses. Let's explore some of these metrics and how they are calculated using the standard binary classification confusion matrix:\n",
    "\n",
    "```\n",
    "                 Predicted Negative    Predicted Positive\n",
    "Actual Negative        TN                   FP\n",
    "Actual Positive        FN                   TP\n",
    "```\n",
    "\n",
    "**1. Accuracy**:\n",
    "Accuracy measures the overall correctness of the model's predictions across all classes.\n",
    "\n",
    "Accuracy is calculated as:\n",
    "```\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "\n",
    "**2. Precision**:\n",
    "Precision measures the ability of the model to correctly identify positive predictions among all predicted positive instances.\n",
    "\n",
    "Precision is calculated as:\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "\n",
    "**3. Recall (Sensitivity or True Positive Rate)**:\n",
    "Recall measures the ability of the model to correctly identify positive instances among all actual positive instances.\n",
    "\n",
    "Recall is calculated as:\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "**4. Specificity (True Negative Rate)**:\n",
    "Specificity measures the ability of the model to correctly identify negative instances among all actual negative instances.\n",
    "\n",
    "Specificity is calculated as:\n",
    "```\n",
    "Specificity = TN / (TN + FP)\n",
    "```\n",
    "\n",
    "**5. F1 Score**:\n",
    "The F1 score is the harmonic mean of precision and recall and provides a balanced measure between the two.\n",
    "\n",
    "F1 Score is calculated as:\n",
    "```\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "**6. False Positive Rate (FPR)**:\n",
    "The false positive rate measures the proportion of negative instances that were incorrectly classified as positive.\n",
    "\n",
    "False Positive Rate is calculated as:\n",
    "```\n",
    "FPR = FP / (FP + TN)\n",
    "```\n",
    "\n",
    "**7. False Negative Rate (FNR)**:\n",
    "The false negative rate measures the proportion of positive instances that were incorrectly classified as negative.\n",
    "\n",
    "False Negative Rate is calculated as:\n",
    "```\n",
    "FNR = FN / (FN + TP)\n",
    "```\n",
    "\n",
    "**8. True Positive Rate (TPR) or Sensitivity**:\n",
    "The true positive rate is another name for recall, measuring the proportion of positive instances that were correctly classified as positive.\n",
    "\n",
    "True Positive Rate is calculated as:\n",
    "```\n",
    "TPR = Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "These metrics provide a comprehensive evaluation of the classification model's performance, considering aspects like correctness, completeness, and false predictions. Depending on the specific problem and the desired focus on precision or recall, different metrics can be prioritized. For example, in medical diagnosis, recall might be more important to reduce false negatives, while in spam email detection, precision might be prioritized to reduce false positives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "\n",
    "\n",
    "ANS-9\n",
    "\n",
    "\n",
    "\n",
    "The accuracy of a model is closely related to the values in its confusion matrix. The confusion matrix provides the raw counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions made by the model. These values are used to calculate the accuracy of the model and other performance metrics.\n",
    "\n",
    "The accuracy of a classification model is a measure of how often the model's predictions are correct overall, considering both positive and negative classes. It is calculated as the ratio of the sum of true positive and true negative predictions to the total number of predictions made by the model.\n",
    "\n",
    "Mathematically, the accuracy (Acc) is calculated as:\n",
    "\n",
    "```\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "\n",
    "Now, let's see how the values in the confusion matrix contribute to the accuracy of the model:\n",
    "\n",
    "- **True Positives (TP)**: These are the instances of the positive class that the model correctly predicted as positive. TP contributes positively to the accuracy since these are the correct positive predictions.\n",
    "\n",
    "- **True Negatives (TN)**: These are the instances of the negative class that the model correctly predicted as negative. TN also contributes positively to the accuracy since these are the correct negative predictions.\n",
    "\n",
    "- **False Positives (FP)**: These are the instances of the negative class that the model incorrectly predicted as positive. FP contributes negatively to the accuracy since these are incorrect predictions.\n",
    "\n",
    "- **False Negatives (FN)**: These are the instances of the positive class that the model incorrectly predicted as negative. FN also contributes negatively to the accuracy since these are incorrect predictions.\n",
    "\n",
    "In summary, the accuracy of the model is affected by the balance between true positive and true negative predictions (correct predictions) and false positive and false negative predictions (incorrect predictions). A higher number of true positives and true negatives relative to false positives and false negatives will result in a higher accuracy.\n",
    "\n",
    "It is important to note that accuracy alone may not be sufficient to evaluate a model's performance, especially when dealing with imbalanced datasets. For example, in a scenario where the positive class is much smaller than the negative class, a model that always predicts the majority class (negative) may achieve high accuracy but fail to correctly predict the positive class (low recall). In such cases, it is essential to consider other performance metrics like precision, recall, F1 score, etc., to get a more comprehensive evaluation of the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
